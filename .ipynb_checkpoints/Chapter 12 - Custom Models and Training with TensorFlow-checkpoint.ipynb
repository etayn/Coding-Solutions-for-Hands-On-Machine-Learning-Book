{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260a9a1e",
   "metadata": {},
   "source": [
    "## Chapter 12 - Custom Models and Training with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaae3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f577c1",
   "metadata": {},
   "source": [
    "### question 12 - Creating Custom Layer That Performs Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ee227",
   "metadata": {},
   "source": [
    "i subclass the layer class of Keras, and defining the build and call functions which are called in the ____ call ____ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd263800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNormLayer(keras.layers.Layer):\n",
    "    def __init__(self , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.alpha = self.add_weight(name = \"alpha\" , shape=input_shape[-1:] , dtype=tf.float32,\n",
    "                                    initializer=keras.initializers.Ones)\n",
    "        self.beta = self.add_weight(name = \"beta\" , shape=input_shape[-1:] , dtype=tf.float32,\n",
    "                                   initializer=keras.initializers.Zeros)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self,X):\n",
    "        mu , var = tf.nn.moments(X , axes=-1 , keepdims=True)\n",
    "        sigma = tf.sqrt(var)\n",
    "        return (self.alpha*(X-mu)/(sigma + 0.001)) + self.beta\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1d01a",
   "metadata": {},
   "source": [
    "### Question 13 - Training a Model Using Custom Training Loop on Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1048bc",
   "metadata": {},
   "source": [
    "For this question, i'm going to create 3 custom training loops:\n",
    "\n",
    "- The first is written entirly inside the loop\n",
    "- The second is written with functions and @tf.function \n",
    "- The third is subclassing keras.Model class, and overwriting the train_step function of the class\n",
    "\n",
    "when in each training method i evaluate also on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa8215",
   "metadata": {},
   "source": [
    "The purpose of making those 3 loops is to compare the training time between the loop versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df841fe6",
   "metadata": {},
   "source": [
    "getting the fasion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3194285",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train , y_train) , (X_test , y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5157368",
   "metadata": {},
   "source": [
    "standardise the data, splitting in into train and validation, and transforming it to keras.Dataset object with 32 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13bcf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255 \n",
    "X_test =  X_test.astype(\"float32\") / 255\n",
    "X_train , X_val , y_train , y_val = train_test_split(X_train , y_train , train_size=0.8)\n",
    "training_data = tf.data.Dataset.from_tensor_slices((X_train , y_train)).batch(32)\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_val , y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a72851",
   "metadata": {},
   "source": [
    "function to compose a basic model, with flattening layer at the start, followed by dense and dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d73f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(28,28))\n",
    "    flatten = keras.layers.Flatten()(inputs)\n",
    "    x = keras.layers.Dense(512 , activation = \"relu\")(flatten)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(100 , activation = \"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    output = keras.layers.Dense(10 , activation = \"softmax\")(x)\n",
    "    model = keras.Model(inputs , output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeaba9",
   "metadata": {},
   "source": [
    "defining the model configuration - loss function to calculate gradients from, optimizer to update the model weights, \n",
    "metric to follow and a metric to update the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1cf0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307ec7",
   "metadata": {},
   "source": [
    "First custom training loop - for each epoch i'm reseting the state of the metric and the loss, and then for each batch in the training data i'm feedfoward the inputs into the model and calculating the loss. then i'm calculate the gradients with regard to each trainable weight in the model and with the optimizer i'm updating the new weights. \n",
    "After finishing going through all the training data i'm updating the logs dict for each metric, again reseting the metric states and calculating those matric with regard to the validation data and again updating the logs dict. after that i print the results at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e726bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of epoch 1:\n",
      " => sparse_categorical_accuracy : 0.7696\n",
      " => loss : 0.6330\n",
      " => val_sparse_categorical_accuracy : 0.8130\n",
      " => val_loss : 0.4775\n",
      "Results of epoch 2:\n",
      " => sparse_categorical_accuracy : 0.8256\n",
      " => loss : 0.4772\n",
      " => val_sparse_categorical_accuracy : 0.8349\n",
      " => val_loss : 0.4315\n",
      "Results of epoch 3:\n",
      " => sparse_categorical_accuracy : 0.8376\n",
      " => loss : 0.4415\n",
      " => val_sparse_categorical_accuracy : 0.8528\n",
      " => val_loss : 0.3897\n",
      "Results of epoch 4:\n",
      " => sparse_categorical_accuracy : 0.8463\n",
      " => loss : 0.4177\n",
      " => val_sparse_categorical_accuracy : 0.8492\n",
      " => val_loss : 0.3976\n",
      "Results of epoch 5:\n",
      " => sparse_categorical_accuracy : 0.8517\n",
      " => loss : 0.4031\n",
      " => val_sparse_categorical_accuracy : 0.8678\n",
      " => val_loss : 0.3632\n",
      "Results of epoch 6:\n",
      " => sparse_categorical_accuracy : 0.8587\n",
      " => loss : 0.3914\n",
      " => val_sparse_categorical_accuracy : 0.8698\n",
      " => val_loss : 0.3608\n",
      "Results of epoch 7:\n",
      " => sparse_categorical_accuracy : 0.8617\n",
      " => loss : 0.3763\n",
      " => val_sparse_categorical_accuracy : 0.8643\n",
      " => val_loss : 0.3603\n",
      "Results of epoch 8:\n",
      " => sparse_categorical_accuracy : 0.8653\n",
      " => loss : 0.3696\n",
      " => val_sparse_categorical_accuracy : 0.8737\n",
      " => val_loss : 0.3449\n",
      "Results of epoch 9:\n",
      " => sparse_categorical_accuracy : 0.8683\n",
      " => loss : 0.3587\n",
      " => val_sparse_categorical_accuracy : 0.8782\n",
      " => val_loss : 0.3393\n",
      "Results of epoch 10:\n",
      " => sparse_categorical_accuracy : 0.8669\n",
      " => loss : 0.3594\n",
      " => val_sparse_categorical_accuracy : 0.8773\n",
      " => val_loss : 0.3436\n",
      "Time taken for training : 120.24791383743286\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "    loss_tracking_metric.reset_states()\n",
    "    \n",
    "    for inputs_batch , labels_batch in training_data:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs_batch , training = True)\n",
    "            loss = loss_fn(labels_batch , predictions)\n",
    "        gradients = tape.gradient(loss , model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients , model.trainable_weights))\n",
    "    \n",
    "        logs = {}\n",
    "        for metric in metrics:\n",
    "            metric.update_state(labels_batch , predictions)\n",
    "            logs[metric.name] = metric.result()\n",
    "        loss_tracking_metric.update_state(loss)\n",
    "        logs[\"loss\"] = loss_tracking_metric.result()\n",
    "        \n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "    loss_tracking_metric.reset_states()\n",
    "        \n",
    "    for val_inputs_batch , val_label_batch in validation_data:\n",
    "        predictions = model(val_inputs_batch , training = False)\n",
    "        loss = loss_fn(val_label_batch , predictions)\n",
    "        for metric in metrics:\n",
    "            metric.update_state(val_label_batch , predictions)\n",
    "            logs[\"val_\" + metric.name] = metric.result()\n",
    "        loss_tracking_metric.update_state(loss)\n",
    "        logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "            \n",
    "        \n",
    "    print(f\"Results of epoch {epoch + 1}:\")\n",
    "    for key , value in logs.items():\n",
    "        print(f\" => {key} : {value:.4f}\")\n",
    "        \n",
    "end_time = time.time()\n",
    "print(\"Time taken for training :\" , end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e222a3",
   "metadata": {},
   "source": [
    "The first training loop took us a whooping __2 minutes!__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8009ba",
   "metadata": {},
   "source": [
    "Defining functions for the second custom training loop - i creat a function to reset the states of the metrics, function that updating the logs dict differently if we in the training stage or validation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443b3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_resetting():\n",
    "    for metric in  metrics:\n",
    "        metric.reset_states()\n",
    "    loss_tracking_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b9153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updating_logs(labels , preds, loss , is_training):\n",
    "    if is_training:\n",
    "        for metric in metrics:\n",
    "            metric.update_state(labels , preds)\n",
    "            logs[metric.name] = metric.result()\n",
    "        loss_tracking_metric.update_state(loss)\n",
    "        logs[\"loss\"] = loss_tracking_metric.result()\n",
    "        \n",
    "    else:\n",
    "        for metric in metrics:\n",
    "            metric.update_state(labels , preds)\n",
    "            logs[\"val_\" + metric.name] = metric.result()\n",
    "        loss_tracking_metric.update_state(loss)\n",
    "        logs[\"val_loss\"] = loss_tracking_metric.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabc763",
   "metadata": {},
   "source": [
    "Defining the training stage and passing it into @tf.function in order to make it computanional graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a126cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_iteration(inputs,labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs , training = True)\n",
    "        loss = loss_fn(labels , predictions)\n",
    "    gradients = tape.gradient(loss , model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients , model.trainable_weights))\n",
    "    return predictions , loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9b907",
   "metadata": {},
   "source": [
    "Doing the same with the validation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43703882",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_iteration(inputs,labels):\n",
    "    predictions = model(inputs , training = False)\n",
    "    loss = loss_fn(labels , predictions)\n",
    "    return predictions , loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918cde1",
   "metadata": {},
   "source": [
    "Second Custom training loop - the same as the first one but with predefined @tf.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb86aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of epoch 1:\n",
      " => sparse_categorical_accuracy : 0.7654\n",
      " => loss : 0.6395\n",
      " => val_sparse_categorical_accuracy : 0.8256\n",
      " => val_loss : 0.4515\n",
      "Results of epoch 2:\n",
      " => sparse_categorical_accuracy : 0.8263\n",
      " => loss : 0.4788\n",
      " => val_sparse_categorical_accuracy : 0.8477\n",
      " => val_loss : 0.4133\n",
      "Results of epoch 3:\n",
      " => sparse_categorical_accuracy : 0.8369\n",
      " => loss : 0.4457\n",
      " => val_sparse_categorical_accuracy : 0.8574\n",
      " => val_loss : 0.3907\n",
      "Results of epoch 4:\n",
      " => sparse_categorical_accuracy : 0.8449\n",
      " => loss : 0.4198\n",
      " => val_sparse_categorical_accuracy : 0.8670\n",
      " => val_loss : 0.3718\n",
      "Results of epoch 5:\n",
      " => sparse_categorical_accuracy : 0.8509\n",
      " => loss : 0.4075\n",
      " => val_sparse_categorical_accuracy : 0.8647\n",
      " => val_loss : 0.3683\n",
      "Results of epoch 6:\n",
      " => sparse_categorical_accuracy : 0.8573\n",
      " => loss : 0.3911\n",
      " => val_sparse_categorical_accuracy : 0.8683\n",
      " => val_loss : 0.3527\n",
      "Results of epoch 7:\n",
      " => sparse_categorical_accuracy : 0.8615\n",
      " => loss : 0.3793\n",
      " => val_sparse_categorical_accuracy : 0.8663\n",
      " => val_loss : 0.3523\n",
      "Results of epoch 8:\n",
      " => sparse_categorical_accuracy : 0.8619\n",
      " => loss : 0.3718\n",
      " => val_sparse_categorical_accuracy : 0.8709\n",
      " => val_loss : 0.3484\n",
      "Results of epoch 9:\n",
      " => sparse_categorical_accuracy : 0.8646\n",
      " => loss : 0.3676\n",
      " => val_sparse_categorical_accuracy : 0.8792\n",
      " => val_loss : 0.3332\n",
      "Results of epoch 10:\n",
      " => sparse_categorical_accuracy : 0.8660\n",
      " => loss : 0.3615\n",
      " => val_sparse_categorical_accuracy : 0.8747\n",
      " => val_loss : 0.3388\n",
      "Time taken for training : 56.83210349082947\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metrics_resetting()\n",
    "    logs = {}\n",
    "    \n",
    "    for inputs_batch , labels_batch in training_data:\n",
    "        predictions , loss = training_iteration(inputs_batch , labels_batch)\n",
    "        updating_logs(labels_batch , predictions , loss , True)\n",
    "        \n",
    "    metrics_resetting()\n",
    "        \n",
    "    for val_inputs_batch , val_labels_batch in validation_data:\n",
    "        predictions , loss = validation_iteration(val_inputs_batch , val_labels_batch)    \n",
    "        updating_logs(val_labels_batch , predictions , loss , False)\n",
    "        \n",
    "    print(f\"Results of epoch {epoch + 1}:\")\n",
    "    for key , value in logs.items():\n",
    "        print(f\" => {key} : {value:.4f}\")\n",
    "        \n",
    "end_time = time.time()\n",
    "print(\"Time taken for training :\" , end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f2ef4",
   "metadata": {},
   "source": [
    "We can see the power of @tf.function here in the second training loop - just by wrapping the training into a @tf.function, it shortens the time of training by more than half to __56 seconds!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a47ee4",
   "metadata": {},
   "source": [
    "Third custom training loop - subclassing the keras.Model class, and inside overwriting the train_step function of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc65de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.compiled_loss(targets, predictions) \n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        self.compiled_metrics.update_state(targets, predictions) \n",
    "        return {m.name: m.result() for m in self.metrics} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a2a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28,28))\n",
    "flatten = keras.layers.Flatten()(inputs)\n",
    "x = keras.layers.Dense(512 , activation = \"relu\")(flatten)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(100 , activation = \"relu\")(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "output = keras.layers.Dense(10 , activation = \"softmax\")(x)\n",
    "model = MyCustomModel(inputs , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f69b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6307 - sparse_categorical_accuracy: 0.7891 - val_loss: 0.4482 - val_sparse_categorical_accuracy: 0.8443\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4764 - sparse_categorical_accuracy: 0.8276 - val_loss: 0.3987 - val_sparse_categorical_accuracy: 0.8564\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4423 - sparse_categorical_accuracy: 0.8376 - val_loss: 0.3890 - val_sparse_categorical_accuracy: 0.8538\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8470 - val_loss: 0.3735 - val_sparse_categorical_accuracy: 0.8652\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4039 - sparse_categorical_accuracy: 0.8522 - val_loss: 0.3683 - val_sparse_categorical_accuracy: 0.8698\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3897 - sparse_categorical_accuracy: 0.8577 - val_loss: 0.3638 - val_sparse_categorical_accuracy: 0.8632\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3781 - sparse_categorical_accuracy: 0.8596 - val_loss: 0.3585 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3740 - sparse_categorical_accuracy: 0.8620 - val_loss: 0.3464 - val_sparse_categorical_accuracy: 0.8754\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3652 - sparse_categorical_accuracy: 0.8651 - val_loss: 0.3459 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3593 - sparse_categorical_accuracy: 0.8682 - val_loss: 0.3326 - val_sparse_categorical_accuracy: 0.8765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2832a42a100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer ,\n",
    "             loss = loss_fn,\n",
    "             metrics= metrics)\n",
    "model.fit(X_train , y_train , batch_size=32 , epochs=10 ,\n",
    "         validation_data=(X_val , y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d973ab",
   "metadata": {},
   "source": [
    "For the third training loop is not even a cometition, only about __30 seconds__ to train!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
